# Android-Malware-Detection
Final Dissertation for Msc in Data Analytics - NCI , Dublin

## Objective
This project examined two unique, image and text-based malware detection
strategies before proposing novel classification techniques. It is clear from previous
research in the field that feature engineering is crucial for producing high-quality
training datasets. This study mainly focused on feature engineering techniques for
proper classification of android malwares with the data obtained from either Dex or
manifest files, or both. While transforming the malware detection challenge to a text
processing problem, a novel approach was identified, proposed and tested. Along
with it, another innovative technique was developed in the image-based approach.
Both the approaches demonstrated encouraging results in terms of classifier accuracy, precision, recall, and F1 score. Although, the dataset was highly unbalanced,
the text-based models attained approx. 95% accuracy. On the other hand, using
the same dataset, the image-based models had an accuracy rate of about 84%.


##Methodology
The methods employed in the project to study end-to-end techniques for Android Malware
Detection are explained in this section. The key topics covered are, general overview of
the chosen dataset, data pre-processing steps, model architectures, and the malware
detection procedure. These details are explained in the sub-sections 3.1 and 3.2 below.

### Dataset
The dataset (Taheri et al. (2019)) selected for this research was collected from the official
website of Canadian Institute of Cybersecurity at UNB (University of New Brunswick).
It includes 1596 Apk files with the most current samples belonging from the year 2020.
The 6 samples are divided into six malware categories, with each category are then further
divided into their respective malware families. The primary categories, which comprises
up to 496 Apk files, are Riskware, Adware, Banking Malware, SMS Malware, and Banking
Malware. The remaining 1100 Apk files are the benign samples that were gathered from
the years 2015, 2016, and 2017.

### KDD Steps
The project used the KDD (Knowledge Discovery in Databases) technique, which is
illustrated in Figure 1 along with the details in the subsections below.

*A. Data Collection*

In this step the dataset was downloaded from official website of Canadian Institute of
Cybersecurity. The APK files in the downloaded dataset were already organized into zip
files according to their categories. These zip files were extracted to a local filesystem and
made accessible to project code for further analysis.

*B. Data Pre-processing*

In this pre-processing step the downloaded zip files were dynamically unpacked into suitable location. Next the APK files were renamed using a hashing algorithm in order to
uniquely identify them in future steps of the project. The available file information for
each APK were then captured and organized into a logical tabular format for further
analysis and experiments.

*C. Data Transformation*

During this step the unpacked APK files were further decompiled into its constituent
components using specialized software. These components were then filtered and chosen
based on the proposed direction of the research. Finally, the chosen components were then
transformed, using specialized strategies, into formats suitable for deep neural networks
employed in the project.

*D.  Data Mining*

In this step, advanced Image and text based neural network models were employed to
automatically learn the latent semantic features from within the huge amount of generated
input data. Apart from the feature extraction, the models were also useful in classifying
the input data into its appropriate classes.

*E. Evaluation*

The proposed methodologies and models were evaluated in this step. Several experiments
were carried out and their results compared based on accuracy, precision, recall, F1 scores
and their Inference times. Apart from that, the models from both text and image-based
approaches were compared as well. Line graphs were generated using the validation
accuracy and losses against the epochs for the each one of the implemented neural network
models to track the training progress. AUC/ROC curves for each of the models were also
generated to better understand their performances. Details on these are provided in the
Section 6.

*F. Interpretation*

Several scenarios were simulated in the experiments and their performances were recorded
during the evaluation phase. In this step, these results were compared to extract critical
knowledge.


## Design Specification

This project mainly focused on Static analysis of the APK files. This technique of analysis
is governed by the thorough study of the application behaviour through its source code.
The objective is twofold viz

+ Misuse Detection (resource misuse)
+ Anomaly Identification (behavioural anomaly) in the target application.

Unlike traditional machine learning approaches, the end-to-end deep learning approach, adopted in this project automatically learns latent features within the source code
of the application to achieve the above objectives. The quality of this learning mostly
depends upon two core things namely App characterization and Neural Network
Model implementation. A brief description of the Android App Characterization and
Model Implementation is provided in the sub-sections 4.1 and 4.2 respectively for clear
understanding.

### Android App Characterization

In the paper(QiuJunyang et al. (2020)) the researchers have defined “App Characterization” as the systematic process of properly representing an application to the appropriate neural network models. They have also listed two broad ways of characterizing an
application, widely adopted by most researchers. These are

+ Dalvik bytecode sequences from “classes.dex” and meta information from the
“AndroidManifest.xml” file for sequential analysis.

+ Dalvik bytecode translated to RGB values to generate images for advanced image
processing.

Both the ways have been used for the experiments in this project. They have been
described in detail in the following subsections.

*App Characterization for Text based Analysis:*

Dalvik Bytecode/Opcode sequences from “classes.dex” files were chosen to represent an application for this task. Ideally, the cumulative number of all opcodes extracted
from the Dex files of an application may range somewhere between 1k to 200k. If each
opcode is 8 considered as a single token, the number is too large to be max sequence
length for sentiment analysis models like, LSTM. Moreover, plain splitting of the large
sequences into multiple sequence may introduce abnormal behaviour into the language
models. Under these circumstances, an intelligent technique was adopted to generate sequences of length between 100 – 500 tokens without losing much information. This novel
technique was named as Opcode Sequence Sampling. The step-by-step description
of the strategy is provided below along with its illustration in the Figure 2.

+ **Step 1 : Grouping -**
In the Dex files the opcodes are organized as classes and
methods. A class contains a State and a Behaviour. A State is represented by
variables, fields and constants, whereas the behaviour by its methods/api. These
apis are basically, blocks of basic instructions or opcodes as defined in the android
official3 website. In this step these opcodes were grouped together based on their
parent api and a list were generated. Essentially, the process converted each api
into a token.
+ **Step 2 : Filtering -**
In this step duplicate methods were removed from the list to
reduce redundancy.
+ **Step 3 : Splitting and Shuffling -**
The filtered method list was then split into
four equal parts and a new list was formed with each splitted list as its entries.
After each split operation the list of lists is shuffled.
+ **Step 4 : Sampling -**
 In this step random samples are retrieved from the split
lists and concatenated to form a single sequence. This is illustrated in the figure 2
above.

*App Characterization for Image based Analysis:*

The basic idea is to generate RGB images using characteristic features of the application.
The process adopted to generate these images is depicted in Figure 3. There are two
parts to this process as described below.

+ The first one revolves around the strategy behind transforming string values to the
corresponding colour pixels. This was done by encoding them into ASCII values
and taking their sum. Then modulus operation determined the corresponding pixel
for the string.

+ The second part employed the colour encoding strategy for generating pixel values
for red, green and blue channels using the application information. These generated colour channels were then merged using an image processing library to form
the final image. Also, during merging, an interpolation algorithm called “Nearest
Neighbour Interpolation” are applied on the channels to unify the dimensions
of all the channels. Some examples of the generated image files are provided below
in the Figure 4. In a novel technique adopted in this research each of the colour
channels encoded one distinct feature of the application. The Red channel was used
to encode all the external API calls that doesn’t appear in the opcodes. The Blue
channel comprised only opcodes. The green channel captured all the important
information available and accessible from the manifest file. This information constitutes all the permissions, intents, libraries, features, and meta data of the android
components. This channel also encoded all the string values from the application.
The final image was then stored in an array prior to feeding them to the image
classification models.

### Model Implementation and Hyper-parameter Tuning
Three language-based models were used for malware classifications, and those are Logistic Regression, LSTM and Bidirectional-LSTM. The tokenization and text embeddings for these models were obtained using CountVectorizer, Tokenizer and Word2Vec.
For Image based classification one state-of-the-art pre-trained model was chosen among
several based on its parameter size, accuracy and its suitability for running on handheld devices and that was EfficientNetB4. A detailed comparison of the most popular
Pre-Trained Image models, suitable for running on handheld devices are provided in
the Table 1 below. Also, a CNN model was designed for making a comparison to
the chosen pre-trained model. Finally, a novel bandit based hyper-parameter tuning
algorithm namely ”HyperBand” proposed in the paper Li et al. (2017) was used for
tuning all the models.

## Implementation
This section discusses the implementation details of experiments conducted in the research. It begins with a brief anatomy of a typical APK file. Consecutively it lists the
tools used for decompilation of these files and for transforming the data extracted from
it. Before that it also describes the development environment used in the research. Finally, it discusses the implementation details of the app characterization, neural network
models and hyperparameter tuning.

### Anatomy of an APK:
APK is a compressed android executable file for installing applications in an android
device. This file is generated and signed using Android App Packaging Tool (AAPT)
and jar signer respectively. AAPT is responsible for packaging all the compiled source
code, UI resources, manifest file and checksum certificates into a single .apk file format as
shown in Figure 5. Also shown in the same figure contents of a typical decompressed apk.
A decompressed apk usually consists of 3 important files viz AndroidManifest.xml,
classes.dex, resources.arsc and 4 subfolders namely assets, lib, META-INF and
res. This project makes use of mainly two files, namely “classes.dex” and “AndroidManifest.xml”. The details of these files are given below:

+ **Classes.dex:** Dex stands for Dalvik Executable. This file consists of all the java
source code compiled into Dalvik bytecode by DX compiler as shown in the figure
above.

+ **AndroidManifest.xml:** Also known as the manifest file is a xml file, which holds
all the meta information like application name, package name, list of required permissions, names of the important components of the applications like activities,
services, receivers etc. This file also defines the entry points of the application and
details of various action based intent navigation between different components of
the app.

### Development Environment:
The following software platforms and tools were employed to carry out the research’s
experiments:

+ **OS:** Windows 11 Home Single Language
+ **Language:** Python
+ **Development Tool:** Jupyter Notebook
+ **Package Manager:** Conda Environment
+ **Versioning:** Git
+ **Repository:** GitHub
+ **Cloud Resource:** Google Collaboratory & Google Drive

###  Important Libraries used in the project

+ **AndroGuard:** Interactive and API base python tool for APK analysis and decompilation
+ **OpenCV:** Versatile image processing library used in Computer Vision applications.
+ **TensorFlow:** Data Pre-processing, Machine Learning and Deep Learning Library
+ **Gensim:** Natural Language Processing Library

### Data Pre-processing:
Before the APK files were decompiled, they were dynamically extracted from the compressed zip files and renamed with their md5 digest.This was done to uniquely identify
those files throughout the research implementation.

### Decompilation:
One of the most important tasks in the project was to extract “classes.dex” and “AndroidManifest.xml” files from the target APK. This was achieved by a powerful tool
named “AndroGuard”. It was used to dynamically decompile the apk files into its
constituent component files described in the Section 5.1. It was also used to extract the
opcodes and all relevant information from the Dex and manifest file respectively. This
extracted information was then used to generate text sequences for NLP and RGB images
for Image processing-based analysis respectively. These two strategies are discussed in
the following subsections 5.6 and 5.7.

### NLP Based Strategy

*A. Preparing Input:*
Using “AndroGuard” opcode names from the application dex files were extracted dynamically and stored in their respective text file. Next using the novel Opcode Sequence
Sampling technique described in the Section 4.1.1, the original sequences were reduced
to smaller ones. A corpus was then generated with all the transformed opcode sequences
and vocabulary extracted for training the word embedding models. CountVectorizer
from Scikit learn and Word2Vec from Gensim were then used to train the respective
embedding spaces. Simultaneously all the sequences were tokenized either manually or
automatically using Keras Tokenizer. To unify the length of the sequences across all
the applications necessary padding and masking were added. Finally, these uniform sequences were passed through the embedding layer to generate respective word embeddings
suitable to feed onto the next layer of DL models. In case of the Word2Vec embeddings
the weights were used to initialize the Keras Embedding Layer.

*B. Model Implementation:*
The architecture for the basic unit of LSTM and Bi-LSTM models are depicted in the
Figure 6 below. Both the models were implemented using Keras APIs.

###  Image Based Strategy

*A. Preparing Input:*
After the decompilation, the opcodes, external api calls and manifest information were
dynamically retrieved using “AndroGuard” and stored in their respective python lists.
During the next step. these lists were converted into a list of respective pixels values
using the technique mentioned in Section 4.1.2. These lists were then transformed into
their respective 2D matrixes and then subjected to interpolation before getting merged
into one single image file. Both the tasks of interpolation and merging was handled by a
specialized library called OpenCV. The image files were then converted to 3D Numpy
array and stored as .npy file prior to feeding them to the image classification models.

*B. Model implementation:*
Two models out of which one base CNN model and one state-of-art models were implemented during the experiments. The pre-trained model that was employed in the
experiments was EfficientNetB4. The pre-trained model was selected based on multiple criteria namely, model size, number of parameters and accuracy as can be
seen in Table 1 under Section 4. The architecture overview for a typical EfficientNet is
provided in the Figure 7.

## Evaluation
In this section, the neural network and machine learning models employed during the
experiments are compared and evaluated using various metrics. The dataset was split
into training, validation, and a test set for the evaluation. The different metrics calculated
are Accuracy, Precision, Recall, F1 score, AUC/ROC and Inference Time as displayed in
Table 2 and 3.

### Model Outcomes
The models implemented in the project classifies whether an android application sample
is either malicious or not, which makes it a two class problem. Malicious applications
are labelled as ”1” and belongs to the “Positive” class whereas the benign applications
are labelled as ”0” and belongs to the “Negative” class. As a result, any benign
application predicted as ”1” can be referred as “False Positives” and any malicious
applications predicted as ”0” as “False Negatives”. Similarly, the opposite outcomes
are respectively ”True Negatives” and ”True Positives”.

### Performance Comparison of ML & DL models used in both NLP & Image based experiments
All the models are compared on the basis of performance scores and inference time as
explained in the Sub-Sections 6.2.1 and 6.2.2 below.

*A. Scores Comparison*

Among all the metrics the most important for performance measurement in this project is
”Sensitivity” or ”Recall” of the models along with the ”AUC/ROC” score. This is
due to the fact that it is very important that all the models correctly predict the maximum
True Positives out of all the actual positives while displaying improved capability for
correctly separating the classes. The models need not be very accurate when predicting
actual negatives.

Following observations are made by comparing the ”Sensitivity”, ”AUC/ROC”
and the generated plots for the models:

+ Out of all the models, the Logistic Regression model’s performance paired with the
CountVectorizer was the best as can be seen in the Table 2. The AUC/ROC curve
for the model is provided in the Figure 8.

+ It is also clear from the table that the LSTM models without word2vec embeddings perform significantly better than the rest. The plots for the two best NLP
based neural network models are provided below in the Figure 9 and 10 respectively.
The left plot in both the figures displays the AUC/ROC curve. The top and
the bottom plots on the right side of the figures show the learning accuracy
and the loss of the model during the training phase.

+ When compared to text-based models, the evaluation metrics show the image-based
models to perform poorly. EfficientNetB4 and the base CNN model both fell
short of outperforming the most elementary text models. Figure 11 and 12 show the
plots for the best EfficentNetB4 and Base CNN models obtained, respectively.

*B. Inference Time Comparison*

Inference time in deep learning refers to the total time required for a model to perform
one full forward propagation, which results in the production of an output given there
is an input. It is a crucial metric for deep learning model optimization.
This inference time can be deduced using two important metrics viz

+ **FLOPs :** It refers to the total number of Floating-Point Operations performed
by a model during one forward pass.This can be estimated by adding up the occurence of all the floating-point arithmetic operations like addition, subtraction,
multiplication, and division in the target deep learning model.

+ **FLOPS :** It refers to the total number of Floating-Point Operations a piece
of hardware can perform in a second.This can be estimated using data such as
CPU speed, core count, CPU instruction rate, and number of CPUs per node as
explained below:

Finally Inference Time can be calculated as:

Using the PIP package namely “keras-flops”, the FLOPs for all the deep learning
models were calculated, which is displayed in the Table 3 below.

From the above table following observations can be made:

+ The NLP-based methods are the quickest of all the stated solutions. Based on
the Inference Time the fastest language model is the fine-tuned Bi-LSTM Model
using Word2Vec Embeddings with 1.08e-05 G FLOPs.
+ Second Fastest solution would be fined tuned LSTM model using Word2Vec
embeddings with 1.933e-05 G FLOPs.
+ The fastest solution in the image-based approach is the Base CNN model with
0.251 G FLOPs whereas the second fastest is the fine-tuned Base CNN model
with 0.504 G FLOPs.

It can be clearly observed from the Table 2 and Table 3, that Inference Time is
directly proportional to the accuracy of the deep learning models.

## Conclusion and Future Work
The metrics above show that the language-based models outperformed the image models
in terms of performance. It should be emphasized that the CNN models were trained
with additional data from manifest files and external APIs, but the language-based models
were trained exclusively on opcode data. Therefore, it is safe to say that the innovative
opcode sequence sampling technique used in this research to characterize the applications
performed very well. Additionally, it demonstrates how crucial, feature selection from the
decompiled APK files is to malware detection. By, generating more samples of the opcode
sequences, there is still room to enhance the performance of the language models.In the
future, further experiments can be conducted by concatenating together the best models
from both the approaches.